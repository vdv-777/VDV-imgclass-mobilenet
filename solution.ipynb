{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd6fa5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üÜï –ö–æ–Ω—Ñ–∏–≥ —Å–æ–∑–¥–∞–Ω: config.yaml\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –ø—Ä–æ–µ–∫—Ç–∞: –∑–∞–≥—Ä—É–∑–∫–∞ –∏–∑ —Ñ–∞–π–ª–∞ –∏–ª–∏ —Å–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤–æ–≥–æ\n",
    "# =============================================================================\n",
    "import os\n",
    "import yaml\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader, Subset, random_split\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import glob\n",
    "\n",
    "CONFIG_FILE = \"config.yaml\"\n",
    "\n",
    "DEFAULT_CONFIG = {\n",
    "    \"debug_mode\": False,\n",
    "    \"debug_train_size\": 500,\n",
    "    \"debug_val_size\": 100,\n",
    "    \"data_dir\": \"ogyeiv2\",\n",
    "    \"batch_size\": 32,\n",
    "    \"epochs\": 10,\n",
    "    \"learning_rate\": 1e-3,\n",
    "    \"img_size\": 224,\n",
    "    \"inference_dir\": \"vdv-imgclass\",\n",
    "    \"model_save_path\": \"meds_classifier.pt\",\n",
    "    \"inference_output_csv\": \"vdv-imgclass.csv\",\n",
    "    \"target_val_acc\": 75.0,\n",
    "    \"dataset_structure\": \"split\"  # \"split\" –∏–ª–∏ \"flat\"\n",
    "}\n",
    "\n",
    "def load_or_create_config(config_file=CONFIG_FILE):\n",
    "    if os.path.exists(config_file):\n",
    "        with open(config_file, 'r', encoding='utf-8') as f:\n",
    "            config = yaml.safe_load(f)\n",
    "        print(f\"‚úÖ –ö–æ–Ω—Ñ–∏–≥ –∑–∞–≥—Ä—É–∂–µ–Ω –∏–∑ {config_file}\")\n",
    "    else:\n",
    "        config = DEFAULT_CONFIG.copy()\n",
    "        with open(config_file, 'w', encoding='utf-8') as f:\n",
    "            yaml.dump(config, f, default_flow_style=False, allow_unicode=True)\n",
    "        print(f\"üÜï –ö–æ–Ω—Ñ–∏–≥ —Å–æ–∑–¥–∞–Ω: {config_file}\")\n",
    "    return config\n",
    "\n",
    "CONFIG = load_or_create_config()\n",
    "CLASS_NAMES = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3b656d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Ä–∞–∑–¥–µ–ª—ë–Ω–Ω–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ (train/test)\n",
      "‚úÖ –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Å–æ–≤: 84\n",
      "‚úÖ Train: 500 –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π\n",
      "‚úÖ Val: 100 –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π\n",
      "‚úÖ –ü—Ä–∏–º–µ—Ä—ã –∫–ª–∞—Å—Å–æ–≤: ['acc_long_600_mg', 'advil_ultra_forte', 'akineton_2_mg', 'algoflex_forte_dolo_400_mg', 'algoflex_rapid_400_mg']\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# –≠—Ç–∞–ø 1. –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö.\n",
    "# –ü–æ–¥–¥–µ—Ä–∂–∫–∞ –¥–≤—É—Ö —Å—Ç—Ä—É–∫—Ç—É—Ä: \"split\" (train/test) –∏ \"flat\" (–µ–¥–∏–Ω—ã–π –Ω–∞–±–æ—Ä)\n",
    "# =============================================================================\n",
    "\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize((CONFIG[\"img_size\"], CONFIG[\"img_size\"])),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(10),\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize((CONFIG[\"img_size\"], CONFIG[\"img_size\"])),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "if CONFIG[\"dataset_structure\"] == \"split\":\n",
    "    print(\"üìÅ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Ä–∞–∑–¥–µ–ª—ë–Ω–Ω–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ (train/test)\")\n",
    "    train_dataset_raw = datasets.ImageFolder(root=os.path.join(CONFIG[\"data_dir\"], \"train\"))\n",
    "    val_dataset_raw = datasets.ImageFolder(root=os.path.join(CONFIG[\"data_dir\"], \"test\"))\n",
    "    assert train_dataset_raw.classes == val_dataset_raw.classes, \"–ö–ª–∞—Å—Å—ã –≤ train –∏ test –Ω–µ —Å–æ–≤–ø–∞–¥–∞—é—Ç!\"\n",
    "    CLASS_NAMES = train_dataset_raw.classes\n",
    "    CONFIG[\"num_classes\"] = len(CLASS_NAMES)\n",
    "\n",
    "    train_dataset = datasets.ImageFolder(\n",
    "        root=os.path.join(CONFIG[\"data_dir\"], \"train\"),\n",
    "        transform=data_transforms['train']\n",
    "    )\n",
    "    val_dataset = datasets.ImageFolder(\n",
    "        root=os.path.join(CONFIG[\"data_dir\"], \"test\"),\n",
    "        transform=data_transforms['val']\n",
    "    )\n",
    "\n",
    "elif CONFIG[\"dataset_structure\"] == \"flat\":\n",
    "    print(\"üìÅ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ø–ª–æ—Å–∫–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ (–µ–¥–∏–Ω—ã–π –Ω–∞–±–æ—Ä –∫–ª–∞—Å—Å–æ–≤)\")\n",
    "    full_dataset_original = datasets.ImageFolder(root=CONFIG[\"data_dir\"])\n",
    "    CLASS_NAMES = full_dataset_original.classes\n",
    "    CONFIG[\"num_classes\"] = len(CLASS_NAMES)\n",
    "\n",
    "    # –°–æ–∑–¥–∞—ë–º –¥–∞—Ç–∞—Å–µ—Ç —Å train-—Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞–º–∏\n",
    "    full_dataset_train = datasets.ImageFolder(root=CONFIG[\"data_dir\"], transform=data_transforms['train'])\n",
    "    # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–æ–≤\n",
    "    val_size = int(0.2 * len(full_dataset_train))\n",
    "    train_size = len(full_dataset_train) - val_size\n",
    "    train_indices, val_indices = random_split(full_dataset_train, [train_size, val_size], generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "    train_dataset = Subset(full_dataset_train, train_indices.indices)\n",
    "    # –í–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç ‚Äî —Å val-—Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞–º–∏\n",
    "    full_dataset_val = datasets.ImageFolder(root=CONFIG[\"data_dir\"], transform=data_transforms['val'])\n",
    "    val_dataset = Subset(full_dataset_val, val_indices.indices)\n",
    "\n",
    "else:\n",
    "    raise ValueError(f\"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞: {CONFIG['dataset_structure']}. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ 'split' –∏–ª–∏ 'flat'.\")\n",
    "\n",
    "# –î–µ–±–∞–≥-—Ä–µ–∂–∏–º\n",
    "if CONFIG[\"debug_mode\"]:\n",
    "    train_dataset = Subset(train_dataset, range(min(CONFIG[\"debug_train_size\"], len(train_dataset))))\n",
    "    val_dataset = Subset(val_dataset, range(min(CONFIG[\"debug_val_size\"], len(val_dataset))))\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=CONFIG[\"batch_size\"], shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_dataset, batch_size=CONFIG[\"batch_size\"], shuffle=False, num_workers=2)\n",
    "\n",
    "print(f\"‚úÖ –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Å–æ–≤: {CONFIG['num_classes']}\")\n",
    "print(f\"‚úÖ Train: {len(train_dataset)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π\")\n",
    "print(f\"‚úÖ Val: {len(val_dataset)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π\")\n",
    "print(f\"‚úÖ –ü—Ä–∏–º–µ—Ä—ã –∫–ª–∞—Å—Å–æ–≤: {CLASS_NAMES[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c064b7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dvorobkalo\\Desktop\\DS\\LogModel\\PCProjects\\PythonAPK\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\dvorobkalo\\Desktop\\DS\\LogModel\\PCProjects\\PythonAPK\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V3_Small_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V3_Small_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MobileNetV3(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2dNormActivation(\n",
       "      (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "      (2): Hardswish()\n",
       "    )\n",
       "    (1): InvertedResidual(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=16, bias=False)\n",
       "          (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (1): SqueezeExcitation(\n",
       "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (fc2): Conv2d(8, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (activation): ReLU()\n",
       "          (scale_activation): Hardsigmoid()\n",
       "        )\n",
       "        (2): Conv2dNormActivation(\n",
       "          (0): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): InvertedResidual(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(16, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(72, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=72, bias=False)\n",
       "          (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Conv2dNormActivation(\n",
       "          (0): Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): InvertedResidual(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(24, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(88, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(88, 88, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=88, bias=False)\n",
       "          (1): BatchNorm2d(88, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Conv2dNormActivation(\n",
       "          (0): Conv2d(88, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (4): InvertedResidual(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(96, 96, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=96, bias=False)\n",
       "          (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (2): SqueezeExcitation(\n",
       "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (fc2): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (activation): ReLU()\n",
       "          (scale_activation): Hardsigmoid()\n",
       "        )\n",
       "        (3): Conv2dNormActivation(\n",
       "          (0): Conv2d(96, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (5): InvertedResidual(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n",
       "          (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (2): SqueezeExcitation(\n",
       "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(240, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (fc2): Conv2d(64, 240, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (activation): ReLU()\n",
       "          (scale_activation): Hardsigmoid()\n",
       "        )\n",
       "        (3): Conv2dNormActivation(\n",
       "          (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (6): InvertedResidual(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n",
       "          (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (2): SqueezeExcitation(\n",
       "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(240, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (fc2): Conv2d(64, 240, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (activation): ReLU()\n",
       "          (scale_activation): Hardsigmoid()\n",
       "        )\n",
       "        (3): Conv2dNormActivation(\n",
       "          (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (7): InvertedResidual(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)\n",
       "          (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (2): SqueezeExcitation(\n",
       "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(120, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (fc2): Conv2d(32, 120, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (activation): ReLU()\n",
       "          (scale_activation): Hardsigmoid()\n",
       "        )\n",
       "        (3): Conv2dNormActivation(\n",
       "          (0): Conv2d(120, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(48, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (8): InvertedResidual(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(48, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(144, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(144, 144, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=144, bias=False)\n",
       "          (1): BatchNorm2d(144, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (2): SqueezeExcitation(\n",
       "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(144, 40, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (fc2): Conv2d(40, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (activation): ReLU()\n",
       "          (scale_activation): Hardsigmoid()\n",
       "        )\n",
       "        (3): Conv2dNormActivation(\n",
       "          (0): Conv2d(144, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(48, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (9): InvertedResidual(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(288, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(288, 288, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=288, bias=False)\n",
       "          (1): BatchNorm2d(288, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (2): SqueezeExcitation(\n",
       "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(288, 72, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (fc2): Conv2d(72, 288, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (activation): ReLU()\n",
       "          (scale_activation): Hardsigmoid()\n",
       "        )\n",
       "        (3): Conv2dNormActivation(\n",
       "          (0): Conv2d(288, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (10): InvertedResidual(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(576, 576, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=576, bias=False)\n",
       "          (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (2): SqueezeExcitation(\n",
       "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(576, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (fc2): Conv2d(144, 576, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (activation): ReLU()\n",
       "          (scale_activation): Hardsigmoid()\n",
       "        )\n",
       "        (3): Conv2dNormActivation(\n",
       "          (0): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (11): InvertedResidual(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(576, 576, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=576, bias=False)\n",
       "          (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (2): SqueezeExcitation(\n",
       "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(576, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (fc2): Conv2d(144, 576, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (activation): ReLU()\n",
       "          (scale_activation): Hardsigmoid()\n",
       "        )\n",
       "        (3): Conv2dNormActivation(\n",
       "          (0): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (12): Conv2dNormActivation(\n",
       "      (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "      (2): Hardswish()\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=576, out_features=1024, bias=True)\n",
       "    (1): Hardswish()\n",
       "    (2): Dropout(p=0.2, inplace=True)\n",
       "    (3): Linear(in_features=1024, out_features=84, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# –≠—Ç–∞–ø 2. –û–±—ä—è–≤–ª–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏\n",
    "# =============================================================================\n",
    "\n",
    "model = torchvision.models.mobilenet_v3_small(pretrained=True)\n",
    "\n",
    "# –ó–∞–º–æ—Ä–æ–∑–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤—ã—Ö —Å–ª–æ—ë–≤\n",
    "for param in model.features.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# –ó–∞–º–µ–Ω–∞ –≥–æ–ª–æ–≤—ã\n",
    "model.classifier[3] = nn.Linear(model.classifier[3].in_features, CONFIG[\"num_classes\"])\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c4f1d8d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: cpu\n",
      "–≠–ø–æ—Ö–∞ 1/10 | Train Loss: 3.0609 | Train Acc: 21.40% | Val Acc: 18.00%\n",
      "–≠–ø–æ—Ö–∞ 2/10 | Train Loss: 1.8448 | Train Acc: 45.60% | Val Acc: 30.00%\n",
      "–≠–ø–æ—Ö–∞ 3/10 | Train Loss: 1.3222 | Train Acc: 61.20% | Val Acc: 28.00%\n",
      "–≠–ø–æ—Ö–∞ 4/10 | Train Loss: 1.1388 | Train Acc: 63.60% | Val Acc: 29.00%\n",
      "–≠–ø–æ—Ö–∞ 5/10 | Train Loss: 0.8986 | Train Acc: 68.80% | Val Acc: 33.00%\n",
      "–≠–ø–æ—Ö–∞ 6/10 | Train Loss: 0.8247 | Train Acc: 74.20% | Val Acc: 36.00%\n",
      "–≠–ø–æ—Ö–∞ 7/10 | Train Loss: 0.7382 | Train Acc: 77.60% | Val Acc: 33.00%\n",
      "–≠–ø–æ—Ö–∞ 8/10 | Train Loss: 0.7104 | Train Acc: 77.40% | Val Acc: 40.00%\n",
      "–≠–ø–æ—Ö–∞ 9/10 | Train Loss: 0.6614 | Train Acc: 76.80% | Val Acc: 37.00%\n",
      "–≠–ø–æ—Ö–∞ 10/10 | Train Loss: 0.5780 | Train Acc: 84.80% | Val Acc: 44.00%\n",
      "–ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ meds_classifier.pt\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# –≠—Ç–∞–ø 3. –û–±—É—á–µ–Ω–∏–µ –∏–ª–∏ –¥–æ–æ–±—É—á–µ–Ω–∏–µ —Å —Ä–∞–Ω–Ω–µ–π –æ—Å—Ç–∞–Ω–æ–≤–∫–æ–π\n",
    "# =============================================================================\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"–£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}\")\n",
    "\n",
    "model = model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.classifier.parameters(), lr=CONFIG[\"learning_rate\"])\n",
    "\n",
    "target_reached = False\n",
    "\n",
    "for epoch in range(CONFIG[\"epochs\"]):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total_train += labels.size(0)\n",
    "        correct_train += (predicted == labels).sum().item()\n",
    "\n",
    "    train_acc = 100 * correct_train / total_train\n",
    "\n",
    "    # –í–∞–ª–∏–¥–∞—Ü–∏—è\n",
    "    model.eval()\n",
    "    correct_val = 0\n",
    "    total_val = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_val += labels.size(0)\n",
    "            correct_val += (predicted == labels).sum().item()\n",
    "\n",
    "    val_acc = 100 * correct_val / total_val\n",
    "    print(f\"–≠–ø–æ—Ö–∞ {epoch+1}/{CONFIG['epochs']} | \"\n",
    "          f\"Train Loss: {running_loss/len(train_loader):.4f} | \"\n",
    "          f\"Train Acc: {train_acc:.2f}% | \"\n",
    "          f\"Val Acc: {val_acc:.2f}%\")\n",
    "\n",
    "    if val_acc >= CONFIG[\"target_val_acc\"]:\n",
    "        print(f\"üéØ –û–±—É—á–µ–Ω–∏–µ –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ –≤ —Å–≤—è–∑–∏ —Å –¥–æ—Å—Ç–∏–∂–µ–Ω–∏–µ–º —Ü–µ–ª–µ–≤–æ–≥–æ –ø–æ—Ä–æ–≥–∞ —Ç–æ—á–Ω–æ—Å—Ç–∏ ({CONFIG['target_val_acc']}%)\")\n",
    "        target_reached = True\n",
    "        break\n",
    "\n",
    "# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –Ω–∞ CPU\n",
    "model = model.cpu()\n",
    "torch.save(model.state_dict(), CONFIG[\"model_save_path\"])\n",
    "print(f\"–ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {CONFIG['model_save_path']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7bc4d850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: cpu\n",
      "\n",
      "============================================================\n",
      "–≠–ø–æ—Ö–∞ 1/10\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "–û–±—É—á–µ–Ω–∏–µ: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:42<00:00,  2.68s/batch, Loss=3.0904, Acc=17.80%]\n",
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìå –ò—Ç–æ–≥–∏ —ç–ø–æ—Ö–∏ 1:\n",
      "  Train Loss: 3.0904 | Train Acc: 17.80%\n",
      "  Val Loss: 3.4314   | Val Acc: 20.00%\n",
      "‚úÖ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ (–ª—É—á—à–∞—è Val Acc: 20.00%) ‚Üí meds_classifier.pt\n",
      "\n",
      "============================================================\n",
      "–≠–ø–æ—Ö–∞ 2/10\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "–û–±—É—á–µ–Ω–∏–µ: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:47<00:00,  2.96s/batch, Loss=1.8670, Acc=44.20%]\n",
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìå –ò—Ç–æ–≥–∏ —ç–ø–æ—Ö–∏ 2:\n",
      "  Train Loss: 1.8670 | Train Acc: 44.20%\n",
      "  Val Loss: 3.1155   | Val Acc: 20.00%\n",
      "\n",
      "============================================================\n",
      "–≠–ø–æ—Ö–∞ 3/10\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "–û–±—É—á–µ–Ω–∏–µ: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:42<00:00,  2.67s/batch, Loss=1.3328, Acc=60.00%]\n",
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìå –ò—Ç–æ–≥–∏ —ç–ø–æ—Ö–∏ 3:\n",
      "  Train Loss: 1.3328 | Train Acc: 60.00%\n",
      "  Val Loss: 2.8912   | Val Acc: 21.00%\n",
      "‚úÖ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ (–ª—É—á—à–∞—è Val Acc: 21.00%) ‚Üí meds_classifier.pt\n",
      "\n",
      "============================================================\n",
      "–≠–ø–æ—Ö–∞ 4/10\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "–û–±—É—á–µ–Ω–∏–µ: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:44<00:00,  2.76s/batch, Loss=1.0955, Acc=64.40%]\n",
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìå –ò—Ç–æ–≥–∏ —ç–ø–æ—Ö–∏ 4:\n",
      "  Train Loss: 1.0955 | Train Acc: 64.40%\n",
      "  Val Loss: 2.7021   | Val Acc: 31.00%\n",
      "‚úÖ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ (–ª—É—á—à–∞—è Val Acc: 31.00%) ‚Üí meds_classifier.pt\n",
      "\n",
      "============================================================\n",
      "–≠–ø–æ—Ö–∞ 5/10\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "–û–±—É—á–µ–Ω–∏–µ: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:42<00:00,  2.66s/batch, Loss=0.9463, Acc=70.80%]\n",
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìå –ò—Ç–æ–≥–∏ —ç–ø–æ—Ö–∏ 5:\n",
      "  Train Loss: 0.9463 | Train Acc: 70.80%\n",
      "  Val Loss: 2.5946   | Val Acc: 31.00%\n",
      "\n",
      "============================================================\n",
      "–≠–ø–æ—Ö–∞ 6/10\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "–û–±—É—á–µ–Ω–∏–µ: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:40<00:00,  2.55s/batch, Loss=0.8287, Acc=73.40%]\n",
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìå –ò—Ç–æ–≥–∏ —ç–ø–æ—Ö–∏ 6:\n",
      "  Train Loss: 0.8287 | Train Acc: 73.40%\n",
      "  Val Loss: 2.3514   | Val Acc: 36.00%\n",
      "‚úÖ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ (–ª—É—á—à–∞—è Val Acc: 36.00%) ‚Üí meds_classifier.pt\n",
      "\n",
      "============================================================\n",
      "–≠–ø–æ—Ö–∞ 7/10\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "–û–±—É—á–µ–Ω–∏–µ: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:52<00:00,  3.27s/batch, Loss=0.7072, Acc=77.20%]\n",
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìå –ò—Ç–æ–≥–∏ —ç–ø–æ—Ö–∏ 7:\n",
      "  Train Loss: 0.7072 | Train Acc: 77.20%\n",
      "  Val Loss: 2.0969   | Val Acc: 50.00%\n",
      "‚úÖ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ (–ª—É—á—à–∞—è Val Acc: 50.00%) ‚Üí meds_classifier.pt\n",
      "\n",
      "============================================================\n",
      "–≠–ø–æ—Ö–∞ 8/10\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "–û–±—É—á–µ–Ω–∏–µ: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:48<00:00,  3.04s/batch, Loss=0.5879, Acc=81.60%]\n",
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìå –ò—Ç–æ–≥–∏ —ç–ø–æ—Ö–∏ 8:\n",
      "  Train Loss: 0.5879 | Train Acc: 81.60%\n",
      "  Val Loss: 1.9429   | Val Acc: 44.00%\n",
      "\n",
      "============================================================\n",
      "–≠–ø–æ—Ö–∞ 9/10\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "–û–±—É—á–µ–Ω–∏–µ: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:45<00:00,  2.86s/batch, Loss=0.6554, Acc=76.20%]\n",
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìå –ò—Ç–æ–≥–∏ —ç–ø–æ—Ö–∏ 9:\n",
      "  Train Loss: 0.6554 | Train Acc: 76.20%\n",
      "  Val Loss: 1.7468   | Val Acc: 43.00%\n",
      "\n",
      "============================================================\n",
      "–≠–ø–æ—Ö–∞ 10/10\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "–û–±—É—á–µ–Ω–∏–µ: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:41<00:00,  2.58s/batch, Loss=0.6047, Acc=81.00%]\n",
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìå –ò—Ç–æ–≥–∏ —ç–ø–æ—Ö–∏ 10:\n",
      "  Train Loss: 0.6047 | Train Acc: 81.00%\n",
      "  Val Loss: 1.7211   | Val Acc: 44.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# –≠—Ç–∞–ø 3. –û–±—É—á–µ–Ω–∏–µ —Å –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä–æ–º (tqdm), –≤—Ä–µ–º–µ–Ω–µ–º –∏ –º–µ—Ç—Ä–∏–∫–∞–º–∏\n",
    "# –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç—Å—è –≤ CONFIG[\"model_save_path\"], —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ Val Acc —É–ª—É—á—à–∏–ª–∞—Å—å\n",
    "# =============================================================================\n",
    "\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"–£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}\")\n",
    "\n",
    "model = torchvision.models.mobilenet_v3_small(pretrained=True)\n",
    "for param in model.features.parameters():\n",
    "    param.requires_grad = False\n",
    "model.classifier[3] = nn.Linear(model.classifier[3].in_features, CONFIG[\"num_classes\"])\n",
    "\n",
    "model = model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.classifier.parameters(), lr=CONFIG[\"learning_rate\"])\n",
    "\n",
    "best_val_acc = 0.0\n",
    "target_reached = False\n",
    "\n",
    "for epoch in range(CONFIG[\"epochs\"]):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"–≠–ø–æ—Ö–∞ {epoch+1}/{CONFIG['epochs']}\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    # ===== –û–±—É—á–µ–Ω–∏–µ =====\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "\n",
    "    # –ò—Å–ø–æ–ª—å–∑—É–µ–º tqdm –¥–ª—è –∫—Ä–∞—Å–∏–≤–æ–≥–æ –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä–∞\n",
    "    pbar = tqdm(train_loader, desc=\"–û–±—É—á–µ–Ω–∏–µ\", unit=\"batch\", leave=True)\n",
    "    \n",
    "    for inputs, labels in pbar:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs,(labels))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total_train += labels.size(0)\n",
    "        correct_train += (predicted == labels).sum().item()\n",
    "\n",
    "        # –¢–µ–∫—É—â–∏–µ –º–µ—Ç—Ä–∏–∫–∏\n",
    "        avg_loss = running_loss / (pbar.n + 1)\n",
    "        train_acc = 100 * correct_train / total_train\n",
    "\n",
    "        # –û–±–Ω–æ–≤–ª—è–µ–º –æ–ø–∏—Å–∞–Ω–∏–µ –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä–∞\n",
    "        pbar.set_postfix({\n",
    "            \"Loss\": f\"{avg_loss:.4f}\",\n",
    "            \"Acc\": f\"{train_acc:.2f}%\"\n",
    "        })\n",
    "\n",
    "    train_acc_final = 100 * correct_train / total_train\n",
    "\n",
    "    # ===== –í–∞–ª–∏–¥–∞—Ü–∏—è =====\n",
    "    model.eval()\n",
    "    correct_val = 0\n",
    "    total_val = 0\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        val_pbar = tqdm(val_loader, desc=\"–í–∞–ª–∏–¥–∞—Ü–∏—è\", unit=\"batch\", leave=False)\n",
    "        for inputs, labels in val_pbar:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_val += labels.size(0)\n",
    "            correct_val += (predicted == labels).sum().item()\n",
    "\n",
    "    val_acc = 100 * correct_val / total_val\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "\n",
    "    print(f\"\\nüìå –ò—Ç–æ–≥–∏ —ç–ø–æ—Ö–∏ {epoch+1}:\")\n",
    "    print(f\"  Train Loss: {running_loss/len(train_loader):.4f} | Train Acc: {train_acc_final:.2f}%\")\n",
    "    print(f\"  Val Loss: {avg_val_loss:.4f}   | Val Acc: {val_acc:.2f}%\")\n",
    "\n",
    "    # ===== –†–∞–Ω–Ω—è—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∞ =====\n",
    "    if val_acc >= CONFIG[\"target_val_acc\"]:\n",
    "        print(f\"üéØ –î–æ—Å—Ç–∏–≥–Ω—É—Ç —Ü–µ–ª–µ–≤–æ–π –ø–æ—Ä–æ–≥ —Ç–æ—á–Ω–æ—Å—Ç–∏ ({CONFIG['target_val_acc']}%) ‚Äî –æ–±—É—á–µ–Ω–∏–µ –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ.\")\n",
    "        target_reached = True\n",
    "\n",
    "    # ===== –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏ =====\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        model.cpu()\n",
    "        torch.save(model.state_dict(), CONFIG[\"model_save_path\"])\n",
    "        print(f\"‚úÖ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ (–ª—É—á—à–∞—è Val Acc: {val_acc:.2f}%) ‚Üí {CONFIG['model_save_path']}\")\n",
    "        model.to(device)\n",
    "\n",
    "    if target_reached:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "18e61ff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–û–±—â–∞—è accuracy: 50.00%\n",
      "\n",
      "–¢–æ–ø-5 –∫–ª–∞—Å—Å–æ–≤ —Å –Ω–∞–∏–±–æ–ª—å—à–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –æ—à–∏–±–æ–∫:\n",
      " - akineton_2_mg: 6 –æ—à–∏–±–æ–∫\n",
      " - algoflex_rapid_400_mg: 6 –æ—à–∏–±–æ–∫\n",
      " - ambroxol_egis_30_mg: 6 –æ—à–∏–±–æ–∫\n",
      " - aspirin_ultra_500_mg: 6 –æ—à–∏–±–æ–∫\n",
      " - betaloc_50_mg: 6 –æ—à–∏–±–æ–∫\n",
      "\n",
      "–¢–æ–ø-5 –∫–ª–∞—Å—Å–æ–≤ —Å –Ω–∞–∏–º–µ–Ω—å—à–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –æ—à–∏–±–æ–∫:\n",
      " - cataflam_dolo_25_mg: 1 –æ—à–∏–±–æ–∫\n",
      " - advil_ultra_forte: 0 –æ—à–∏–±–æ–∫\n",
      " - apranax_550_mg: 0 –æ—à–∏–±–æ–∫\n",
      " - c_vitamin_teva_500_mg: 0 –æ—à–∏–±–æ–∫\n",
      " - calci_kid: 0 –æ—à–∏–±–æ–∫\n",
      "\n",
      "–ö–ª–∞—Å—Å—ã –±–µ–∑ –æ—à–∏–±–æ–∫:\n",
      " - advil_ultra_forte\n",
      " - apranax_550_mg\n",
      " - c_vitamin_teva_500_mg\n",
      " - calci_kid\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# –≠—Ç–∞–ø 4. –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞\n",
    "# =============================================================================\n",
    "\n",
    "model.load_state_dict(torch.load(CONFIG[\"model_save_path\"], map_location='cpu'))\n",
    "model.eval()\n",
    "\n",
    "all_labels = []\n",
    "all_preds = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in val_loader:\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        all_labels.extend(labels.numpy())\n",
    "        all_preds.extend(predicted.numpy())\n",
    "\n",
    "accuracy = accuracy_score(all_labels, all_preds)\n",
    "print(f\"–û–±—â–∞—è accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "error_counts = cm.sum(axis=1) - np.diag(cm)\n",
    "class_error_pairs = list(zip(CLASS_NAMES, error_counts))\n",
    "sorted_by_errors = sorted(class_error_pairs, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"\\n–¢–æ–ø-5 –∫–ª–∞—Å—Å–æ–≤ —Å –Ω–∞–∏–±–æ–ª—å—à–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –æ—à–∏–±–æ–∫:\")\n",
    "for cls, err in sorted_by_errors[:5]:\n",
    "    print(f\" - {cls}: {int(err)} –æ—à–∏–±–æ–∫\")\n",
    "\n",
    "print(\"\\n–¢–æ–ø-5 –∫–ª–∞—Å—Å–æ–≤ —Å –Ω–∞–∏–º–µ–Ω—å—à–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –æ—à–∏–±–æ–∫:\")\n",
    "for cls, err in sorted_by_errors[-5:]:\n",
    "    print(f\" - {cls}: {int(err)} –æ—à–∏–±–æ–∫\")\n",
    "\n",
    "print(\"\\n–ö–ª–∞—Å—Å—ã –±–µ–∑ –æ—à–∏–±–æ–∫:\")\n",
    "zero_error = [cls for cls, err in class_error_pairs if err == 0]\n",
    "if zero_error:\n",
    "    for cls in zero_error:\n",
    "        print(f\" - {cls}\")\n",
    "else:\n",
    "    print(\" - –¢–∞–∫–∏—Ö –∫–ª–∞—Å—Å–æ–≤ –Ω–µ—Ç\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5423ffe4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'CONFIG' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [2], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# =============================================================================\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# –≠—Ç–∞–ø 5. –ò–Ω—Ñ–µ—Ä–µ–Ω—Å\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# =============================================================================\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(\u001b[43mCONFIG\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minference_dir\u001b[39m\u001b[38;5;124m\"\u001b[39m], exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      7\u001b[0m inference_transform \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[0;32m      8\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mResize((CONFIG[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimg_size\u001b[39m\u001b[38;5;124m\"\u001b[39m], CONFIG[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimg_size\u001b[39m\u001b[38;5;124m\"\u001b[39m])),\n\u001b[0;32m      9\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mToTensor(),\n\u001b[0;32m     10\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mNormalize([\u001b[38;5;241m0.485\u001b[39m, \u001b[38;5;241m0.456\u001b[39m, \u001b[38;5;241m0.406\u001b[39m], [\u001b[38;5;241m0.229\u001b[39m, \u001b[38;5;241m0.224\u001b[39m, \u001b[38;5;241m0.225\u001b[39m])\n\u001b[0;32m     11\u001b[0m ])\n\u001b[0;32m     13\u001b[0m image_paths \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     14\u001b[0m     glob\u001b[38;5;241m.\u001b[39mglob(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(CONFIG[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minference_dir\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m)) \u001b[38;5;241m+\u001b[39m\n\u001b[0;32m     15\u001b[0m     glob\u001b[38;5;241m.\u001b[39mglob(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(CONFIG[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minference_dir\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*.jpeg\u001b[39m\u001b[38;5;124m\"\u001b[39m)) \u001b[38;5;241m+\u001b[39m\n\u001b[0;32m     16\u001b[0m     glob\u001b[38;5;241m.\u001b[39mglob(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(CONFIG[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minference_dir\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*.png\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m     17\u001b[0m )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'CONFIG' is not defined"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# –≠—Ç–∞–ø 5. –ò–Ω—Ñ–µ—Ä–µ–Ω—Å\n",
    "# =============================================================================\n",
    "import os\n",
    "os.makedirs(CONFIG[\"inference_dir\"], exist_ok=True)\n",
    "\n",
    "inference_transform = transforms.Compose([\n",
    "    transforms.Resize((CONFIG[\"img_size\"], CONFIG[\"img_size\"])),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "image_paths = (\n",
    "    glob.glob(os.path.join(CONFIG[\"inference_dir\"], \"*.jpg\")) +\n",
    "    glob.glob(os.path.join(CONFIG[\"inference_dir\"], \"*.jpeg\")) +\n",
    "    glob.glob(os.path.join(CONFIG[\"inference_dir\"], \"*.png\"))\n",
    ")\n",
    "\n",
    "if not image_paths:\n",
    "    print(f\"‚ö†Ô∏è –ù–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –≤ {CONFIG['inference_dir']}\")\n",
    "else:\n",
    "    results = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for img_path in image_paths:\n",
    "            try:\n",
    "                img = Image.open(img_path).convert(\"RGB\")\n",
    "                inp = inference_transform(img).unsqueeze(0)\n",
    "                out = model(inp)\n",
    "                prob = torch.softmax(out, dim=1)\n",
    "                conf, idx = torch.max(prob, 1)\n",
    "                cls_name = CLASS_NAMES[idx.item()]\n",
    "                results.append({\"filename\": os.path.basename(img_path), \"class\": cls_name, \"confidence\": conf.item()})\n",
    "                print(f\"{os.path.basename(img_path)} - {cls_name} - {conf.item():.4f}\")\n",
    "            except Exception as e:\n",
    "                print(f\"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ {img_path}: {e}\")\n",
    "\n",
    "    if results:\n",
    "        pd.DataFrame(results).to_csv(CONFIG[\"inference_output_csv\"], index=False)\n",
    "        print(f\"\\n‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {CONFIG['inference_output_csv']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8cb69d24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "advil_ultra_forte_s_037.jpg - advil_ultra_forte - 0.2591\n",
      "akineton_2_mg_u_013.jpg - atoris_20_mg - 0.0864\n",
      "apranax_550_mg_u_010.jpg - apranax_550_mg - 0.2456\n",
      "aspirin_ultra_500_mg_u_014.jpg - atoris_20_mg - 0.1177\n",
      "atoris_20_mg_s_039.jpg - atoris_20_mg - 0.1367\n",
      "betaloc_50_mg_u_005.jpg - atoris_20_mg - 0.0905\n",
      "calci_kid_u_014.jpg - calci_kid - 0.6338\n",
      "cataflam_dolo_25_mg_s_024.jpg - cataflam_dolo_25_mg - 0.2829\n",
      "cataflam_dolo_25_mg_u_012.jpg - cataflam_dolo_25_mg - 0.1158\n",
      "strepsils_u_001.jpg - calci_kid - 0.3898\n",
      "\n",
      "‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ vdv-imgclass.csv\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# –≠—Ç–∞–ø 5. –ò–Ω—Ñ–µ—Ä–µ–Ω—Å (–∞–≤—Ç–æ–Ω–æ–º–Ω—ã–π ‚Äî —Ä–∞–±–æ—Ç–∞–µ—Ç –ø–æ—Å–ª–µ –ø–µ—Ä–µ–∑–∞–ø—É—Å–∫–∞ —è–¥—Ä–∞)\n",
    "# =============================================================================\n",
    "\n",
    "import os\n",
    "import yaml\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# --- –ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –∏–∑ —Ñ–∞–π–ª–∞ ---\n",
    "CONFIG_FILE = \"config.yaml\"\n",
    "with open(CONFIG_FILE, 'r', encoding='utf-8') as f:\n",
    "    CONFIG = yaml.safe_load(f)\n",
    "\n",
    "# --- –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø—É—Ç–µ–π –∏ –∑–∞–≥—Ä—É–∑–∫–∞ —Å–ø–∏—Å–∫–∞ –∫–ª–∞—Å—Å–æ–≤ ---\n",
    "# –î–ª—è flat-—Å—Ç—Ä—É–∫—Ç—É—Ä—ã –∫–ª–∞—Å—Å—ã –±–µ—Ä—É—Ç—Å—è –∏–∑ data_dir\n",
    "# –î–ª—è split ‚Äî –∏–∑ data_dir/train\n",
    "data_dir = CONFIG[\"data_dir\"]\n",
    "if CONFIG[\"dataset_structure\"] == \"split\":\n",
    "    from torchvision.datasets import ImageFolder\n",
    "    train_path = os.path.join(data_dir, \"train\")\n",
    "    assert os.path.exists(train_path), f\"–ü–∞–ø–∫–∞ {train_path} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞\"\n",
    "    CLASS_NAMES = ImageFolder(root=train_path).classes\n",
    "elif CONFIG[\"dataset_structure\"] == \"flat\":\n",
    "    from torchvision.datasets import ImageFolder\n",
    "    assert os.path.exists(data_dir), f\"–ü–∞–ø–∫–∞ {data_dir} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞\"\n",
    "    CLASS_NAMES = ImageFolder(root=data_dir).classes\n",
    "else:\n",
    "    raise ValueError(f\"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞: {CONFIG['dataset_structure']}\")\n",
    "\n",
    "# --- –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å-–¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ ---\n",
    "os.makedirs(CONFIG[\"inference_dir\"], exist_ok=True)\n",
    "\n",
    "# --- –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º—ã ---\n",
    "inference_transform = transforms.Compose([\n",
    "    transforms.Resize((CONFIG[\"img_size\"], CONFIG[\"img_size\"])),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# --- –ü–æ–∏—Å–∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π ---\n",
    "image_paths = (\n",
    "    glob.glob(os.path.join(CONFIG[\"inference_dir\"], \"*.jpg\")) +\n",
    "    glob.glob(os.path.join(CONFIG[\"inference_dir\"], \"*.jpeg\")) +\n",
    "    glob.glob(os.path.join(CONFIG[\"inference_dir\"], \"*.png\"))\n",
    ")\n",
    "\n",
    "if not image_paths:\n",
    "    print(f\"‚ö†Ô∏è –ù–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –≤ {CONFIG['inference_dir']}\")\n",
    "else:\n",
    "    # --- –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ ---\n",
    "    model = torchvision.models.mobilenet_v3_small(pretrained=False)\n",
    "    model.classifier[3] = torch.nn.Linear(model.classifier[3].in_features, len(CLASS_NAMES))\n",
    "    model.load_state_dict(torch.load(CONFIG[\"model_save_path\"], map_location='cpu'))\n",
    "    model.eval()\n",
    "\n",
    "    results = []\n",
    "    with torch.no_grad():\n",
    "        for img_path in image_paths:\n",
    "            try:\n",
    "                img = Image.open(img_path).convert(\"RGB\")\n",
    "                inp = inference_transform(img).unsqueeze(0)\n",
    "                out = model(inp)\n",
    "                prob = torch.softmax(out, dim=1)\n",
    "                conf, idx = torch.max(prob, 1)\n",
    "                cls_name = CLASS_NAMES[idx.item()]\n",
    "                results.append({\n",
    "                    \"filename\": os.path.basename(img_path),\n",
    "                    \"class\": cls_name,\n",
    "                    \"confidence\": conf.item()\n",
    "                })\n",
    "                print(f\"{os.path.basename(img_path)} - {cls_name} - {conf.item():.4f}\")\n",
    "            except Exception as e:\n",
    "                print(f\"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ {img_path}: {e}\")\n",
    "\n",
    "    if results:\n",
    "        pd.DataFrame(results).to_csv(CONFIG[\"inference_output_csv\"], index=False)\n",
    "        print(f\"\\n‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {CONFIG['inference_output_csv']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PythonAPK (3.10.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
