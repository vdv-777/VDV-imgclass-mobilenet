{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3fd6fa5e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üÜï –ö–æ–Ω—Ñ–∏–≥ —Å–æ–∑–¥–∞–Ω: config.yaml\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –ø—Ä–æ–µ–∫—Ç–∞: –∑–∞–≥—Ä—É–∑–∫–∞ –∏–∑ —Ñ–∞–π–ª–∞ –∏–ª–∏ —Å–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤–æ–≥–æ\n",
    "# =============================================================================\n",
    "import os\n",
    "import yaml\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader, Subset, random_split\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import glob\n",
    "\n",
    "CONFIG_FILE = \"config.yaml\"\n",
    "\n",
    "DEFAULT_CONFIG = {\n",
    "    \"debug_mode\": False,\n",
    "    \"debug_train_size\": 500,\n",
    "    \"debug_val_size\": 100,\n",
    "    \"data_dir\": \"ogyeiv2\",\n",
    "    \"batch_size\": 32,\n",
    "    \"epochs\": 10,\n",
    "    \"learning_rate\": 1e-3,\n",
    "    \"img_size\": 224,\n",
    "    \"inference_dir\": \"vdv-imgclass\",\n",
    "    \"model_save_path\": \"meds_classifier.pt\",\n",
    "    \"inference_output_csv\": \"vdv-imgclass.csv\",\n",
    "    \"target_val_acc\": 75.0,\n",
    "    \"dataset_structure\": \"split\"  # \"split\" –∏–ª–∏ \"flat\"\n",
    "}\n",
    "\n",
    "def load_or_create_config(config_file=CONFIG_FILE):\n",
    "    if os.path.exists(config_file):\n",
    "        with open(config_file, 'r', encoding='utf-8') as f:\n",
    "            config = yaml.safe_load(f)\n",
    "        print(f\"‚úÖ –ö–æ–Ω—Ñ–∏–≥ –∑–∞–≥—Ä—É–∂–µ–Ω –∏–∑ {config_file}\")\n",
    "    else:\n",
    "        config = DEFAULT_CONFIG.copy()\n",
    "        with open(config_file, 'w', encoding='utf-8') as f:\n",
    "            yaml.dump(config, f, default_flow_style=False, allow_unicode=True)\n",
    "        print(f\"üÜï –ö–æ–Ω—Ñ–∏–≥ —Å–æ–∑–¥–∞–Ω: {config_file}\")\n",
    "    return config\n",
    "\n",
    "CONFIG = load_or_create_config()\n",
    "CLASS_NAMES = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b3b656d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Ä–∞–∑–¥–µ–ª—ë–Ω–Ω–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ (train/test)\n",
      "‚úÖ –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Å–æ–≤: 84\n",
      "‚úÖ Train: 2352 –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π\n",
      "‚úÖ Val: 504 –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π\n",
      "‚úÖ –ü—Ä–∏–º–µ—Ä—ã –∫–ª–∞—Å—Å–æ–≤: ['acc_long_600_mg', 'advil_ultra_forte', 'akineton_2_mg', 'algoflex_forte_dolo_400_mg', 'algoflex_rapid_400_mg']\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# –≠—Ç–∞–ø 1. –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö.\n",
    "# –ü–æ–¥–¥–µ—Ä–∂–∫–∞ –¥–≤—É—Ö —Å—Ç—Ä—É–∫—Ç—É—Ä: \"split\" (train/test) –∏ \"flat\" (–µ–¥–∏–Ω—ã–π –Ω–∞–±–æ—Ä)\n",
    "# =============================================================================\n",
    "\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize((CONFIG[\"img_size\"], CONFIG[\"img_size\"])),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(10),\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize((CONFIG[\"img_size\"], CONFIG[\"img_size\"])),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "if CONFIG[\"dataset_structure\"] == \"split\":\n",
    "    print(\"üìÅ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Ä–∞–∑–¥–µ–ª—ë–Ω–Ω–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ (train/test)\")\n",
    "    train_dataset_raw = datasets.ImageFolder(root=os.path.join(CONFIG[\"data_dir\"], \"train\"))\n",
    "    val_dataset_raw = datasets.ImageFolder(root=os.path.join(CONFIG[\"data_dir\"], \"test\"))\n",
    "    assert train_dataset_raw.classes == val_dataset_raw.classes, \"–ö–ª–∞—Å—Å—ã –≤ train –∏ test –Ω–µ —Å–æ–≤–ø–∞–¥–∞—é—Ç!\"\n",
    "    CLASS_NAMES = train_dataset_raw.classes\n",
    "    CONFIG[\"num_classes\"] = len(CLASS_NAMES)\n",
    "\n",
    "    train_dataset = datasets.ImageFolder(\n",
    "        root=os.path.join(CONFIG[\"data_dir\"], \"train\"),\n",
    "        transform=data_transforms['train']\n",
    "    )\n",
    "    val_dataset = datasets.ImageFolder(\n",
    "        root=os.path.join(CONFIG[\"data_dir\"], \"test\"),\n",
    "        transform=data_transforms['val']\n",
    "    )\n",
    "\n",
    "elif CONFIG[\"dataset_structure\"] == \"flat\":\n",
    "    print(\"üìÅ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ø–ª–æ—Å–∫–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ (–µ–¥–∏–Ω—ã–π –Ω–∞–±–æ—Ä –∫–ª–∞—Å—Å–æ–≤)\")\n",
    "    full_dataset_original = datasets.ImageFolder(root=CONFIG[\"data_dir\"])\n",
    "    CLASS_NAMES = full_dataset_original.classes\n",
    "    CONFIG[\"num_classes\"] = len(CLASS_NAMES)\n",
    "\n",
    "    # –°–æ–∑–¥–∞—ë–º –¥–∞—Ç–∞—Å–µ—Ç —Å train-—Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞–º–∏\n",
    "    full_dataset_train = datasets.ImageFolder(root=CONFIG[\"data_dir\"], transform=data_transforms['train'])\n",
    "    # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–æ–≤\n",
    "    val_size = int(0.2 * len(full_dataset_train))\n",
    "    train_size = len(full_dataset_train) - val_size\n",
    "    train_indices, val_indices = random_split(full_dataset_train, [train_size, val_size], generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "    train_dataset = Subset(full_dataset_train, train_indices.indices)\n",
    "    # –í–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç ‚Äî —Å val-—Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞–º–∏\n",
    "    full_dataset_val = datasets.ImageFolder(root=CONFIG[\"data_dir\"], transform=data_transforms['val'])\n",
    "    val_dataset = Subset(full_dataset_val, val_indices.indices)\n",
    "\n",
    "else:\n",
    "    raise ValueError(f\"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞: {CONFIG['dataset_structure']}. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ 'split' –∏–ª–∏ 'flat'.\")\n",
    "\n",
    "# –î–µ–±–∞–≥-—Ä–µ–∂–∏–º\n",
    "if CONFIG[\"debug_mode\"]:\n",
    "    train_dataset = Subset(train_dataset, range(min(CONFIG[\"debug_train_size\"], len(train_dataset))))\n",
    "    val_dataset = Subset(val_dataset, range(min(CONFIG[\"debug_val_size\"], len(val_dataset))))\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=CONFIG[\"batch_size\"], shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_dataset, batch_size=CONFIG[\"batch_size\"], shuffle=False, num_workers=2)\n",
    "\n",
    "print(f\"‚úÖ –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Å–æ–≤: {CONFIG['num_classes']}\")\n",
    "print(f\"‚úÖ Train: {len(train_dataset)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π\")\n",
    "print(f\"‚úÖ Val: {len(val_dataset)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π\")\n",
    "print(f\"‚úÖ –ü—Ä–∏–º–µ—Ä—ã –∫–ª–∞—Å—Å–æ–≤: {CLASS_NAMES[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c064b7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V3_Small_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V3_Small_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/mobilenet_v3_small-047dcff4.pth\" to /home/ubuntu/.cache/torch/hub/checkpoints/mobilenet_v3_small-047dcff4.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9.83M/9.83M [00:00<00:00, 20.9MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MobileNetV3(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2dNormActivation(\n",
       "      (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "      (2): Hardswish()\n",
       "    )\n",
       "    (1): InvertedResidual(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=16, bias=False)\n",
       "          (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (1): SqueezeExcitation(\n",
       "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (fc2): Conv2d(8, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (activation): ReLU()\n",
       "          (scale_activation): Hardsigmoid()\n",
       "        )\n",
       "        (2): Conv2dNormActivation(\n",
       "          (0): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): InvertedResidual(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(16, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(72, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=72, bias=False)\n",
       "          (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Conv2dNormActivation(\n",
       "          (0): Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): InvertedResidual(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(24, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(88, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(88, 88, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=88, bias=False)\n",
       "          (1): BatchNorm2d(88, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Conv2dNormActivation(\n",
       "          (0): Conv2d(88, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (4): InvertedResidual(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(96, 96, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=96, bias=False)\n",
       "          (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (2): SqueezeExcitation(\n",
       "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (fc2): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (activation): ReLU()\n",
       "          (scale_activation): Hardsigmoid()\n",
       "        )\n",
       "        (3): Conv2dNormActivation(\n",
       "          (0): Conv2d(96, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (5): InvertedResidual(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n",
       "          (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (2): SqueezeExcitation(\n",
       "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(240, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (fc2): Conv2d(64, 240, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (activation): ReLU()\n",
       "          (scale_activation): Hardsigmoid()\n",
       "        )\n",
       "        (3): Conv2dNormActivation(\n",
       "          (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (6): InvertedResidual(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n",
       "          (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (2): SqueezeExcitation(\n",
       "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(240, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (fc2): Conv2d(64, 240, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (activation): ReLU()\n",
       "          (scale_activation): Hardsigmoid()\n",
       "        )\n",
       "        (3): Conv2dNormActivation(\n",
       "          (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (7): InvertedResidual(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)\n",
       "          (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (2): SqueezeExcitation(\n",
       "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(120, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (fc2): Conv2d(32, 120, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (activation): ReLU()\n",
       "          (scale_activation): Hardsigmoid()\n",
       "        )\n",
       "        (3): Conv2dNormActivation(\n",
       "          (0): Conv2d(120, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(48, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (8): InvertedResidual(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(48, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(144, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(144, 144, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=144, bias=False)\n",
       "          (1): BatchNorm2d(144, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (2): SqueezeExcitation(\n",
       "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(144, 40, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (fc2): Conv2d(40, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (activation): ReLU()\n",
       "          (scale_activation): Hardsigmoid()\n",
       "        )\n",
       "        (3): Conv2dNormActivation(\n",
       "          (0): Conv2d(144, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(48, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (9): InvertedResidual(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(288, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(288, 288, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=288, bias=False)\n",
       "          (1): BatchNorm2d(288, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (2): SqueezeExcitation(\n",
       "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(288, 72, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (fc2): Conv2d(72, 288, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (activation): ReLU()\n",
       "          (scale_activation): Hardsigmoid()\n",
       "        )\n",
       "        (3): Conv2dNormActivation(\n",
       "          (0): Conv2d(288, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (10): InvertedResidual(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(576, 576, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=576, bias=False)\n",
       "          (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (2): SqueezeExcitation(\n",
       "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(576, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (fc2): Conv2d(144, 576, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (activation): ReLU()\n",
       "          (scale_activation): Hardsigmoid()\n",
       "        )\n",
       "        (3): Conv2dNormActivation(\n",
       "          (0): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (11): InvertedResidual(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(576, 576, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=576, bias=False)\n",
       "          (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (2): SqueezeExcitation(\n",
       "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(576, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (fc2): Conv2d(144, 576, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (activation): ReLU()\n",
       "          (scale_activation): Hardsigmoid()\n",
       "        )\n",
       "        (3): Conv2dNormActivation(\n",
       "          (0): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (12): Conv2dNormActivation(\n",
       "      (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "      (2): Hardswish()\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=576, out_features=1024, bias=True)\n",
       "    (1): Hardswish()\n",
       "    (2): Dropout(p=0.2, inplace=True)\n",
       "    (3): Linear(in_features=1024, out_features=84, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# –≠—Ç–∞–ø 2. –û–±—ä—è–≤–ª–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏\n",
    "# =============================================================================\n",
    "\n",
    "model = torchvision.models.mobilenet_v3_small(pretrained=True)\n",
    "\n",
    "# –ó–∞–º–æ—Ä–æ–∑–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤—ã—Ö —Å–ª–æ—ë–≤\n",
    "for param in model.features.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# –ó–∞–º–µ–Ω–∞ –≥–æ–ª–æ–≤—ã\n",
    "model.classifier[3] = nn.Linear(model.classifier[3].in_features, CONFIG[\"num_classes\"])\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7bc4d850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: cuda\n",
      "\n",
      "============================================================\n",
      "–≠–ø–æ—Ö–∞ 1/10\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "–û–±—É—á–µ–Ω–∏–µ: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 74/74 [01:37<00:00,  1.32s/batch, Loss=3.8305, Acc=11.73%]\n",
      "                                                                                                                                                                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìå –ò—Ç–æ–≥–∏ —ç–ø–æ—Ö–∏ 1:\n",
      "  Train Loss: 3.8305 | Train Acc: 11.73%\n",
      "  Val Loss: 4.0701   | Val Acc: 2.78%\n",
      "‚úÖ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ (–ª—É—á—à–∞—è Val Acc: 2.78%) ‚Üí meds_classifier.pt\n",
      "\n",
      "============================================================\n",
      "–≠–ø–æ—Ö–∞ 2/10\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "–û–±—É—á–µ–Ω–∏–µ: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 74/74 [01:38<00:00,  1.33s/batch, Loss=2.5974, Acc=33.29%]\n",
      "                                                                                                                                                                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìå –ò—Ç–æ–≥–∏ —ç–ø–æ—Ö–∏ 2:\n",
      "  Train Loss: 2.5974 | Train Acc: 33.29%\n",
      "  Val Loss: 3.7386   | Val Acc: 6.35%\n",
      "‚úÖ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ (–ª—É—á—à–∞—è Val Acc: 6.35%) ‚Üí meds_classifier.pt\n",
      "\n",
      "============================================================\n",
      "–≠–ø–æ—Ö–∞ 3/10\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "–û–±—É—á–µ–Ω–∏–µ: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 74/74 [01:39<00:00,  1.35s/batch, Loss=2.0412, Acc=45.79%]\n",
      "                                                                                                                                                                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìå –ò—Ç–æ–≥–∏ —ç–ø–æ—Ö–∏ 3:\n",
      "  Train Loss: 2.0136 | Train Acc: 45.79%\n",
      "  Val Loss: 3.4294   | Val Acc: 12.30%\n",
      "‚úÖ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ (–ª—É—á—à–∞—è Val Acc: 12.30%) ‚Üí meds_classifier.pt\n",
      "\n",
      "============================================================\n",
      "–≠–ø–æ—Ö–∞ 4/10\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "–û–±—É—á–µ–Ω–∏–µ: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 74/74 [01:38<00:00,  1.33s/batch, Loss=1.7669, Acc=51.11%]\n",
      "                                                                                                                                                                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìå –ò—Ç–æ–≥–∏ —ç–ø–æ—Ö–∏ 4:\n",
      "  Train Loss: 1.7669 | Train Acc: 51.11%\n",
      "  Val Loss: 2.4237   | Val Acc: 27.98%\n",
      "‚úÖ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ (–ª—É—á—à–∞—è Val Acc: 27.98%) ‚Üí meds_classifier.pt\n",
      "\n",
      "============================================================\n",
      "–≠–ø–æ—Ö–∞ 5/10\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "–û–±—É—á–µ–Ω–∏–µ: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 74/74 [01:41<00:00,  1.38s/batch, Loss=1.5506, Acc=55.65%]\n",
      "                                                                                                                                                                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìå –ò—Ç–æ–≥–∏ —ç–ø–æ—Ö–∏ 5:\n",
      "  Train Loss: 1.5506 | Train Acc: 55.65%\n",
      "  Val Loss: 1.6495   | Val Acc: 53.97%\n",
      "‚úÖ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ (–ª—É—á—à–∞—è Val Acc: 53.97%) ‚Üí meds_classifier.pt\n",
      "\n",
      "============================================================\n",
      "–≠–ø–æ—Ö–∞ 6/10\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "–û–±—É—á–µ–Ω–∏–µ: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 74/74 [01:39<00:00,  1.34s/batch, Loss=1.4277, Acc=58.80%]\n",
      "                                                                                                                                                                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìå –ò—Ç–æ–≥–∏ —ç–ø–æ—Ö–∏ 6:\n",
      "  Train Loss: 1.4277 | Train Acc: 58.80%\n",
      "  Val Loss: 1.2841   | Val Acc: 63.49%\n",
      "‚úÖ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ (–ª—É—á—à–∞—è Val Acc: 63.49%) ‚Üí meds_classifier.pt\n",
      "\n",
      "============================================================\n",
      "–≠–ø–æ—Ö–∞ 7/10\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "–û–±—É—á–µ–Ω–∏–µ: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 74/74 [01:42<00:00,  1.38s/batch, Loss=1.2723, Acc=63.22%]\n",
      "                                                                                                                                                                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìå –ò—Ç–æ–≥–∏ —ç–ø–æ—Ö–∏ 7:\n",
      "  Train Loss: 1.2723 | Train Acc: 63.22%\n",
      "  Val Loss: 1.0631   | Val Acc: 68.85%\n",
      "‚úÖ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ (–ª—É—á—à–∞—è Val Acc: 68.85%) ‚Üí meds_classifier.pt\n",
      "\n",
      "============================================================\n",
      "–≠–ø–æ—Ö–∞ 8/10\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "–û–±—É—á–µ–Ω–∏–µ: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 74/74 [01:38<00:00,  1.34s/batch, Loss=1.2127, Acc=63.78%]\n",
      "                                                                                                                                                                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìå –ò—Ç–æ–≥–∏ —ç–ø–æ—Ö–∏ 8:\n",
      "  Train Loss: 1.2127 | Train Acc: 63.78%\n",
      "  Val Loss: 1.0654   | Val Acc: 66.67%\n",
      "\n",
      "============================================================\n",
      "–≠–ø–æ—Ö–∞ 9/10\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "–û–±—É—á–µ–Ω–∏–µ: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 74/74 [01:38<00:00,  1.33s/batch, Loss=1.1625, Acc=65.52%]\n",
      "                                                                                                                                                                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìå –ò—Ç–æ–≥–∏ —ç–ø–æ—Ö–∏ 9:\n",
      "  Train Loss: 1.1625 | Train Acc: 65.52%\n",
      "  Val Loss: 0.9491   | Val Acc: 73.81%\n",
      "‚úÖ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ (–ª—É—á—à–∞—è Val Acc: 73.81%) ‚Üí meds_classifier.pt\n",
      "\n",
      "============================================================\n",
      "–≠–ø–æ—Ö–∞ 10/10\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "–û–±—É—á–µ–Ω–∏–µ: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 74/74 [01:38<00:00,  1.33s/batch, Loss=1.1119, Acc=66.67%]\n",
      "                                                                                                                                                                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìå –ò—Ç–æ–≥–∏ —ç–ø–æ—Ö–∏ 10:\n",
      "  Train Loss: 1.1119 | Train Acc: 66.67%\n",
      "  Val Loss: 0.9540   | Val Acc: 73.81%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# –≠—Ç–∞–ø 3. –û–±—É—á–µ–Ω–∏–µ —Å –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä–æ–º (tqdm), –≤—Ä–µ–º–µ–Ω–µ–º –∏ –º–µ—Ç—Ä–∏–∫–∞–º–∏\n",
    "# –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç—Å—è –≤ CONFIG[\"model_save_path\"], —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ Val Acc —É–ª—É—á—à–∏–ª–∞—Å—å\n",
    "# =============================================================================\n",
    "\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"–£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}\")\n",
    "\n",
    "model = torchvision.models.mobilenet_v3_small(pretrained=True)\n",
    "for param in model.features.parameters():\n",
    "    param.requires_grad = False\n",
    "model.classifier[3] = nn.Linear(model.classifier[3].in_features, CONFIG[\"num_classes\"])\n",
    "\n",
    "model = model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.classifier.parameters(), lr=CONFIG[\"learning_rate\"])\n",
    "\n",
    "best_val_acc = 0.0\n",
    "target_reached = False\n",
    "\n",
    "for epoch in range(CONFIG[\"epochs\"]):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"–≠–ø–æ—Ö–∞ {epoch+1}/{CONFIG['epochs']}\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    # ===== –û–±—É—á–µ–Ω–∏–µ =====\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "\n",
    "    # –ò—Å–ø–æ–ª—å–∑—É–µ–º tqdm –¥–ª—è –∫—Ä–∞—Å–∏–≤–æ–≥–æ –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä–∞\n",
    "    pbar = tqdm(train_loader, desc=\"–û–±—É—á–µ–Ω–∏–µ\", unit=\"batch\", leave=True)\n",
    "    \n",
    "    for inputs, labels in pbar:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs,(labels))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total_train += labels.size(0)\n",
    "        correct_train += (predicted == labels).sum().item()\n",
    "\n",
    "        # –¢–µ–∫—É—â–∏–µ –º–µ—Ç—Ä–∏–∫–∏\n",
    "        avg_loss = running_loss / (pbar.n + 1)\n",
    "        train_acc = 100 * correct_train / total_train\n",
    "\n",
    "        # –û–±–Ω–æ–≤–ª—è–µ–º –æ–ø–∏—Å–∞–Ω–∏–µ –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä–∞\n",
    "        pbar.set_postfix({\n",
    "            \"Loss\": f\"{avg_loss:.4f}\",\n",
    "            \"Acc\": f\"{train_acc:.2f}%\"\n",
    "        })\n",
    "\n",
    "    train_acc_final = 100 * correct_train / total_train\n",
    "\n",
    "    # ===== –í–∞–ª–∏–¥–∞—Ü–∏—è =====\n",
    "    model.eval()\n",
    "    correct_val = 0\n",
    "    total_val = 0\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        val_pbar = tqdm(val_loader, desc=\"–í–∞–ª–∏–¥–∞—Ü–∏—è\", unit=\"batch\", leave=False)\n",
    "        for inputs, labels in val_pbar:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_val += labels.size(0)\n",
    "            correct_val += (predicted == labels).sum().item()\n",
    "\n",
    "    val_acc = 100 * correct_val / total_val\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "\n",
    "    print(f\"\\nüìå –ò—Ç–æ–≥–∏ —ç–ø–æ—Ö–∏ {epoch+1}:\")\n",
    "    print(f\"  Train Loss: {running_loss/len(train_loader):.4f} | Train Acc: {train_acc_final:.2f}%\")\n",
    "    print(f\"  Val Loss: {avg_val_loss:.4f}   | Val Acc: {val_acc:.2f}%\")\n",
    "\n",
    "    # ===== –†–∞–Ω–Ω—è—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∞ =====\n",
    "    if val_acc >= CONFIG[\"target_val_acc\"]:\n",
    "        print(f\"üéØ –î–æ—Å—Ç–∏–≥–Ω—É—Ç —Ü–µ–ª–µ–≤–æ–π –ø–æ—Ä–æ–≥ —Ç–æ—á–Ω–æ—Å—Ç–∏ ({CONFIG['target_val_acc']}%) ‚Äî –æ–±—É—á–µ–Ω–∏–µ –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ.\")\n",
    "        target_reached = True\n",
    "\n",
    "    # ===== –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏ =====\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        model.cpu()\n",
    "        torch.save(model.state_dict(), CONFIG[\"model_save_path\"])\n",
    "        print(f\"‚úÖ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ (–ª—É—á—à–∞—è Val Acc: {val_acc:.2f}%) ‚Üí {CONFIG['model_save_path']}\")\n",
    "        model.to(device)\n",
    "\n",
    "    if target_reached:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "95192cc0-bb82-4014-ae55-3f281273d27d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–û–±—â–∞—è accuracy: 73.81%\n",
      "\n",
      "–¢–æ–ø-5 –∫–ª–∞—Å—Å–æ–≤ —Å –Ω–∞–∏–±–æ–ª—å—à–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –æ—à–∏–±–æ–∫:\n",
      " - atorvastatin_teva_20_mg: 6 –æ—à–∏–±–æ–∫\n",
      " - jutavit_cink: 6 –æ—à–∏–±–æ–∫\n",
      " - concor_10_mg: 5 –æ—à–∏–±–æ–∫\n",
      " - quamatel_40_mg: 5 –æ—à–∏–±–æ–∫\n",
      " - theospirex_150_mg: 5 –æ—à–∏–±–æ–∫\n",
      "\n",
      "–¢–æ–ø-5 –∫–ª–∞—Å—Å–æ–≤ —Å –Ω–∞–∏–º–µ–Ω—å—à–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –æ—à–∏–±–æ–∫:\n",
      " - strepsils: 0 –æ—à–∏–±–æ–∫\n",
      " - urzinol: 0 –æ—à–∏–±–æ–∫\n",
      " - vita_c: 0 –æ—à–∏–±–æ–∫\n",
      " - voltaren_dolo_rapid_25_mg: 0 –æ—à–∏–±–æ–∫\n",
      " - zadex_60_mg: 0 –æ—à–∏–±–æ–∫\n",
      "\n",
      "–ö–ª–∞—Å—Å—ã –±–µ–∑ –æ—à–∏–±–æ–∫:\n",
      " - advil_ultra_forte\n",
      " - akineton_2_mg\n",
      " - algoflex_rapid_400_mg\n",
      " - algopyrin_500_mg\n",
      " - bila_git\n",
      " - calci_kid\n",
      " - cataflam_dolo_25_mg\n",
      " - concor_5_mg\n",
      " - diclopram_75-mg_20-mg\n",
      " - dorithricin_mentol\n",
      " - dulsevia_60_mg\n",
      " - laresin_10_mg\n",
      " - metothyrin_10_mg\n",
      " - mezym_forte_10_000_egyseg\n",
      " - milgamma\n",
      " - naprosyn_250_mg\n",
      " - novo_c_plus\n",
      " - ocutein\n",
      " - salazopyrin_en_500_mg\n",
      " - strepfen_8_75_mg\n",
      " - strepsils\n",
      " - urzinol\n",
      " - vita_c\n",
      " - voltaren_dolo_rapid_25_mg\n",
      " - zadex_60_mg\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# –≠—Ç–∞–ø 4. –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ (—Ä–∞–±–æ—Ç–∞–µ—Ç –≤—Å–µ–≥–¥–∞ ‚Äî –Ω–∞ CPU)\n",
    "# =============================================================================\n",
    "\n",
    "import os\n",
    "import yaml\n",
    "from torchvision import models\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "# –ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥–∞ (–Ω–∞ —Å–ª—É—á–∞–π, –µ—Å–ª–∏ –∑–∞–ø—É—Å–∫–∞–µ—Ç–µ –æ—Ç–¥–µ–ª—å–Ω–æ)\n",
    "with open(\"config.yaml\", 'r', encoding='utf-8') as f:\n",
    "    CONFIG = yaml.safe_load(f)\n",
    "\n",
    "# –ü–æ–ª—É—á–µ–Ω–∏–µ CLASS_NAMES (–∞–Ω–∞–ª–æ–≥–∏—á–Ω–æ –≠—Ç–∞–ø—É 5)\n",
    "from torchvision.datasets import ImageFolder\n",
    "if CONFIG[\"dataset_structure\"] == \"split\":\n",
    "    CLASS_NAMES = ImageFolder(root=os.path.join(CONFIG[\"data_dir\"], \"train\")).classes\n",
    "else:\n",
    "    CLASS_NAMES = ImageFolder(root=CONFIG[\"data_dir\"]).classes\n",
    "\n",
    "# –ü–µ—Ä–µ—Å–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ –Ω–∞ CPU\n",
    "model = models.mobilenet_v3_small(pretrained=False)\n",
    "model.classifier[3] = torch.nn.Linear(model.classifier[3].in_features, len(CLASS_NAMES))\n",
    "model.load_state_dict(torch.load(CONFIG[\"model_save_path\"], map_location='cpu'))\n",
    "model.eval()  # –≤–∞–∂–Ω–æ!\n",
    "\n",
    "# –ó–∞–≥—Ä—É–∑–∫–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞ (–∞–Ω–∞–ª–æ–≥–∏—á–Ω–æ –≠—Ç–∞–ø—É 1, –Ω–æ —Ç–æ–ª—å–∫–æ val)\n",
    "from torchvision import transforms\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((CONFIG[\"img_size\"], CONFIG[\"img_size\"])),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "if CONFIG[\"dataset_structure\"] == \"split\":\n",
    "    val_dataset = ImageFolder(root=os.path.join(CONFIG[\"data_dir\"], \"test\"), transform=val_transform)\n",
    "else:\n",
    "    # –î–ª—è flat-—Å—Ç—Ä—É–∫—Ç—É—Ä—ã –Ω—É–∂–Ω–æ –ø–æ–≤—Ç–æ—Ä–Ω–æ —Ä–∞–∑–¥–µ–ª–∏—Ç—å, –Ω–æ –ø—Ä–æ—â–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å val_loader –∏–∑ train-—Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è\n",
    "    # –û–¥–Ω–∞–∫–æ –¥–ª—è —Ç–æ—á–Ω–æ—Å—Ç–∏ –æ—Ü–µ–Ω–∫–∏ ‚Äî –ª—É—á—à–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π test, –µ—Å–ª–∏ –æ–Ω –µ—Å—Ç—å\n",
    "    # –í –¥–∞–Ω–Ω–æ–º —Å–ª—É—á–∞–µ, –µ—Å–ª–∏ flat ‚Äî –º—ã –Ω–µ –º–æ–∂–µ–º —Ç–æ—á–Ω–æ –≤–æ—Å—Å–æ–∑–¥–∞—Ç—å val –±–µ–∑ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∏–Ω–¥–µ–∫—Å–æ–≤\n",
    "    # –ü–æ—ç—Ç–æ–º—É –ø—Ä–µ–¥–ø–æ–ª–æ–∂–∏–º, —á—Ç–æ –≤ flat —Ä–µ–∂–∏–º–µ test –Ω–µ—Ç, –∏ –≤—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏ random_split\n",
    "    # –ù–æ –¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–π –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ ‚Äî —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å split-—Å—Ç—Ä—É–∫—Ç—É—Ä—É\n",
    "    raise NotImplementedError(\"–û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–ª—è flat-—Å—Ç—Ä—É–∫—Ç—É—Ä—ã —Ç—Ä–µ–±—É–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è val-–∏–Ω–¥–µ–∫—Å–æ–≤. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ dataset_structure: split.\")\n",
    "\n",
    "val_loader = DataLoader(val_dataset, batch_size=CONFIG[\"batch_size\"], shuffle=False, num_workers=2)\n",
    "\n",
    "# –û—Ü–µ–Ω–∫–∞\n",
    "all_labels = []\n",
    "all_preds = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in val_loader:\n",
    "        # inputs –∏ labels —É–∂–µ –Ω–∞ CPU (ImageFolder + DataLoader –±–µ–∑ .to())\n",
    "        outputs = model(inputs)  # –º–æ–¥–µ–ª—å –Ω–∞ CPU ‚Üí –≤—Å—ë –æ–∫\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        all_labels.extend(labels.numpy())\n",
    "        all_preds.extend(predicted.numpy())\n",
    "\n",
    "# –î–∞–ª–µ–µ ‚Äî –∫–∞–∫ —Ä–∞–Ω—å—à–µ\n",
    "accuracy = accuracy_score(all_labels, all_preds)\n",
    "print(f\"–û–±—â–∞—è accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "error_counts = cm.sum(axis=1) - np.diag(cm)\n",
    "class_error_pairs = list(zip(CLASS_NAMES, error_counts))\n",
    "sorted_by_errors = sorted(class_error_pairs, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"\\n–¢–æ–ø-5 –∫–ª–∞—Å—Å–æ–≤ —Å –Ω–∞–∏–±–æ–ª—å—à–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –æ—à–∏–±–æ–∫:\")\n",
    "for cls, err in sorted_by_errors[:5]:\n",
    "    print(f\" - {cls}: {int(err)} –æ—à–∏–±–æ–∫\")\n",
    "\n",
    "print(\"\\n–¢–æ–ø-5 –∫–ª–∞—Å—Å–æ–≤ —Å –Ω–∞–∏–º–µ–Ω—å—à–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –æ—à–∏–±–æ–∫:\")\n",
    "for cls, err in sorted_by_errors[-5:]:\n",
    "    print(f\" - {cls}: {int(err)} –æ—à–∏–±–æ–∫\")\n",
    "\n",
    "print(\"\\n–ö–ª–∞—Å—Å—ã –±–µ–∑ –æ—à–∏–±–æ–∫:\")\n",
    "zero_error = [cls for cls, err in class_error_pairs if err == 0]\n",
    "if zero_error:\n",
    "    for cls in zero_error:\n",
    "        print(f\" - {cls}\")\n",
    "else:\n",
    "    print(\" - –¢–∞–∫–∏—Ö –∫–ª–∞—Å—Å–æ–≤ –Ω–µ—Ç\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67be2d01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ 84 –∫–ª–∞—Å—Å–æ–≤ –∏–∑ classes.txt\n",
      "advil_ultra_forte_s_037.jpg - advil_ultra_forte - 0.9642\n",
      "akineton_2_mg_u_013.jpg - algopyrin_500_mg - 0.8987\n",
      "apranax_550_mg_u_010.jpg - apranax_550_mg - 0.4894\n",
      "aspirin_ultra_500_mg_u_014.jpg - aspirin_ultra_500_mg - 0.8088\n",
      "atoris_20_mg_s_039.jpg - atoris_20_mg - 0.4635\n",
      "betaloc_50_mg_u_005.jpg - betaloc_50_mg - 0.9603\n",
      "calci_kid_u_014.jpg - calci_kid - 0.9997\n",
      "cataflam_dolo_25_mg_s_024.jpg - cataflam_dolo_25_mg - 0.9926\n",
      "cataflam_dolo_25_mg_u_012.jpg - cataflam_dolo_25_mg - 0.9154\n",
      "strepsils_u_001.jpg - strepsils - 0.9748\n",
      "\n",
      "‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ vdv-imgclass.csv\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# –≠—Ç–∞–ø 5. –ò–Ω—Ñ–µ—Ä–µ–Ω—Å (–∞–≤—Ç–æ–Ω–æ–º–Ω—ã–π ‚Äî —Ä–∞–±–æ—Ç–∞–µ—Ç –±–µ–∑ –¥–∞—Ç–∞—Å–µ—Ç–∞!)\n",
    "# –¢—Ä–µ–±—É–µ—Ç —Ç–æ–ª—å–∫–æ: meds_classifier.pt + classes.txt + config.yaml\n",
    "# =============================================================================\n",
    "\n",
    "import os\n",
    "import yaml\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "# --- –ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ ---\n",
    "with open(\"config.yaml\", 'r', encoding='utf-8') as f:\n",
    "    CONFIG = yaml.safe_load(f)\n",
    "\n",
    "# --- –ó–∞–≥—Ä—É–∑–∫–∞ —Å–ø–∏—Å–∫–∞ –∫–ª–∞—Å—Å–æ–≤ –∏–∑ classes.txt ---\n",
    "try:\n",
    "    with open(\"classes.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "        CLASS_NAMES = [line.strip() for line in f if line.strip()]\n",
    "    print(f\"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(CLASS_NAMES)} –∫–ª–∞—Å—Å–æ–≤ –∏–∑ classes.txt\")\n",
    "except FileNotFoundError:\n",
    "    raise FileNotFoundError(\n",
    "        \"–§–∞–π–ª classes.txt –Ω–µ –Ω–∞–π–¥–µ–Ω. \"\n",
    "        \"–°–æ–∑–¥–∞–π—Ç–µ –µ–≥–æ –≤ –∫–æ—Ä–Ω–µ –ø—Ä–æ–µ–∫—Ç–∞ –∏ –∑–∞–ø–∏—à–∏—Ç–µ –≤ –Ω–µ–≥–æ –Ω–∞–∑–≤–∞–Ω–∏—è –∫–ª–∞—Å—Å–æ–≤ –ø–æ –æ–¥–Ω–æ–º—É –Ω–∞ —Å—Ç—Ä–æ–∫—É.\"\n",
    "    )\n",
    "\n",
    "# --- –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å-–¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ ---\n",
    "os.makedirs(CONFIG[\"inference_dir\"], exist_ok=True)\n",
    "\n",
    "# --- –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º—ã ---\n",
    "inference_transform = transforms.Compose([\n",
    "    transforms.Resize((CONFIG[\"img_size\"], CONFIG[\"img_size\"])),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# --- –ü–æ–∏—Å–∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π ---\n",
    "image_paths = (\n",
    "    glob.glob(os.path.join(CONFIG[\"inference_dir\"], \"*.jpg\")) +\n",
    "    glob.glob(os.path.join(CONFIG[\"inference_dir\"], \"*.jpeg\")) +\n",
    "    glob.glob(os.path.join(CONFIG[\"inference_dir\"], \"*.png\"))\n",
    ")\n",
    "\n",
    "if not image_paths:\n",
    "    print(f\"‚ö†Ô∏è –ù–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –≤ {CONFIG['inference_dir']}\")\n",
    "else:\n",
    "    # --- –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ ---\n",
    "    model = torchvision.models.mobilenet_v3_small(pretrained=False)\n",
    "    model.classifier[3] = torch.nn.Linear(model.classifier[3].in_features, len(CLASS_NAMES))\n",
    "    model.load_state_dict(torch.load(CONFIG[\"model_save_path\"], map_location='cpu'))\n",
    "    model.eval()\n",
    "\n",
    "    results = []\n",
    "    with torch.no_grad():\n",
    "        for img_path in image_paths:\n",
    "            try:\n",
    "                img = Image.open(img_path).convert(\"RGB\")\n",
    "                inp = inference_transform(img).unsqueeze(0)\n",
    "                out = model(inp)\n",
    "                prob = torch.softmax(out, dim=1)\n",
    "                conf, idx = torch.max(prob, 1)\n",
    "                cls_name = CLASS_NAMES[idx.item()]\n",
    "                results.append({\n",
    "                    \"filename\": os.path.basename(img_path),\n",
    "                    \"class\": cls_name,\n",
    "                    \"confidence\": conf.item()\n",
    "                })\n",
    "                print(f\"{os.path.basename(img_path)} - {cls_name} - {conf.item():.4f}\")\n",
    "            except Exception as e:\n",
    "                print(f\"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ {img_path}: {e}\")\n",
    "\n",
    "    if results:\n",
    "        pd.DataFrame(results).to_csv(CONFIG[\"inference_output_csv\"], index=False)\n",
    "        print(f\"\\n‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {CONFIG['inference_output_csv']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2757033e",
   "metadata": {},
   "source": [
    "Readme –¥–æ—Å—Ç—É–ø–Ω–æ –Ω–∞ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–∏ –ø–æ —Å—Å—ã–ª–∫–µ: https://github.com/vdv-777/VDV-imgclass-mobilenet\n",
    "\n",
    "## üìä –ò—Ç–æ–≥–∏ –æ–±—É—á–µ–Ω–∏—è –Ω–∞ –ø–æ–ª–Ω–æ–º –¥–∞—Ç–∞—Å–µ—Ç–µ (10 —ç–ø–æ—Ö, CUDA)\n",
    "\n",
    "- **–û–±—â–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å (accuracy): `73.81%`** ‚Äî –º–æ–¥–µ–ª—å –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç —Ä–∞–±–æ—Ç–æ—Å–ø–æ—Å–æ–±–Ω—ã–π, —Ö–æ—Ç—è –∏ –Ω–µ –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç.  \n",
    "- **–°–ª–æ–∂–Ω—ã–µ –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ç–∞–±–ª–µ—Ç–∫–∏**:  \n",
    "  –ù–∞–∏–±–æ–ª—å—à–µ–µ —á–∏—Å–ª–æ –æ—à–∏–±–æ–∫ –¥–æ–ø—É—â–µ–Ω–æ –¥–ª—è `atorvastatin_teva_20_mg` –∏ `jutavit_cink` (–ø–æ 6 –æ—à–∏–±–æ–∫), –∞ —Ç–∞–∫–∂–µ `concor_10_mg`, `quamatel_40_mg`, `theospirex_150_mg` (–ø–æ 5 –æ—à–∏–±–æ–∫). –í–µ—Ä–æ—è—Ç–Ω–æ, —ç—Ç–∏ –ø—Ä–µ–ø–∞—Ä–∞—Ç—ã –≤–∏–∑—É–∞–ª—å–Ω–æ —Å—Ö–æ–∂–∏ —Å –¥—Ä—É–≥–∏–º–∏ –∫–ª–∞—Å—Å–∞–º–∏ –∏–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω—ã –≤ –Ω–µ–æ–¥–Ω–æ–∑–Ω–∞—á–Ω—ã—Ö —Ä–∞–∫—É—Ä—Å–∞—Ö/–æ—Å–≤–µ—â–µ–Ω–∏–∏.  \n",
    "- **–ù–∞–¥—ë–∂–Ω–æ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–µ–º—ã–µ —Ç–∞–±–ª–µ—Ç–∫–∏**:  \n",
    "  **25 –∫–ª–∞—Å—Å–æ–≤** –±—ã–ª–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω—ã **–±–µ–∑ –µ–¥–∏–Ω–æ–π –æ—à–∏–±–∫–∏**, –≤–∫–ª—é—á–∞—è `strepsils`, `urzinol`, `vita_c`, `voltaren_dolo_rapid_25_mg`, `zadex_60_mg` –∏ –¥—Ä—É–≥–∏–µ. –≠—Ç–æ —Å–≤–∏–¥–µ—Ç–µ–ª—å—Å—Ç–≤—É–µ—Ç –æ –≤—ã—Å–æ–∫–æ–π —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏ –∫ –±–æ–ª—å—à–∏–Ω—Å—Ç–≤—É –æ–±—Ä–∞–∑—Ü–æ–≤.  \n",
    "- **–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏**:  \n",
    "  –î–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è –æ–±—â–µ–π accuracy —Ü–µ–ª–µ—Å–æ–æ–±—Ä–∞–∑–Ω–æ –ø—Ä–æ–≤–µ—Å—Ç–∏ –∞–Ω–∞–ª–∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π ¬´–ø—Ä–æ–±–ª–µ–º–Ω—ã—Ö¬ª –∫–ª–∞—Å—Å–æ–≤ ‚Äî –≤–æ–∑–º–æ–∂–Ω–æ, —Ç—Ä–µ–±—É–µ—Ç—Å—è –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è, –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ –∏–ª–∏ –¥–æ–æ–±—É—á–µ–Ω–∏–µ —Å —Ñ–æ–∫—É—Å–æ–º –Ω–∞ –Ω–∞–∏–±–æ–ª–µ–µ —Å–ª–æ–∂–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–∞—Ö."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
